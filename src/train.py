from config import device

# Define the training loop
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader)
    # Get the batch from the dataset
    for batch, (X,y) in enumerate(dataloader):
        # Move data on GPU
        X_gpu = X.to(device)
        y_tensor = y.clone().detach()
        y_gpu = y_tensor.to(device)

        # Compute prediction and loss
        pred = model(X_gpu)
        loss = loss_fn(pred, y_gpu)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # Print loss during training
        if batch % 500 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f'loss: {loss} [{current:}/{size}]')

def save_model(model):
    model_name = 'model_new_dataset.pth'
    # Save the model
    torch.save(model.state_dict(), model_name)
    print(f'Saved PyTorch Model State to {model_name}')

if __name__ == "__main__":
    from torch import nn
    import torch
    from utils.load_dataset import load_train_dataset
    from cnn import OurCNN
    
    from utils.load_dataset import load_test_dataset
    from test import test_loop
    test_dataloader = load_test_dataset()

    train_dataloader = load_train_dataset()
    # Model initialization
    model = OurCNN().to(device)

    # Train the model

    # How many times our model analyzes the dataset
    epochs = 3

    # Speed of learning (The smaller the rate, the finer and slower the training will be)
    learning_rate = 0.001

    # Define the loss function
    loss_fn = nn.CrossEntropyLoss()

    # Define the optimizer (Function that computes the gradients for every layer)
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

    print("[+] Training the model")
    # Train the model
    for t in range(epochs):
        print(f'Epoch: {t}')
        train_loop(train_dataloader, model, loss_fn, optimizer)
        print('Testing the model...')
        test_loop(test_dataloader, model)
            
    save_model(model)