import torch
from torch.utils.data import Dataset
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
from torch import nn
from torch.utils.data import DataLoader

from torchvision.datasets import ImageFolder
from pathlib import Path


# Create a DataLoader class
train_path = Path(__file__).parent / "data/train"
test_path = Path(__file__).parent / "data/test"

train_dataset = ImageFolder(root=train_path, transform=ToTensor())
test_dataset = ImageFolder(root=test_path, transform=ToTensor())

train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)

# Create the model 
device = ('cpu')

# Define the CNN
class OurCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 5, 3),
            nn.ReLU(),
            nn.MaxPool2d(3,3),            
            nn.Conv2d(5, 10, 3),
            nn.ReLU(),        
            nn.MaxPool2d(3,3),
            nn.Conv2d(10, 2, 3),
            nn.ReLU(),
            nn.MaxPool2d(3,3),
        )
        self.mlp = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 2)   
        )

    def forward(self, x):
        x = self.cnn(x)
        x = torch.flatten(x,1)
        x = self.mlp(x)
        return x


# Model initialization
model = OurCNN().to(device)

# Train the model

# How many times our model analyzes the dataset
epochs = 3
# Number of samples that we get from the dataset each time
batch_size = 10
# Speed of learning (The smaller the rate, the finer and slower the training will be)
learning_rate = 0.001

# Define the loss function
loss_fn = nn.CrossEntropyLoss()

# Define the optimizer (Function that computes the gradients for every layer)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

# Define the training loop
def train_loop(dataloader, model, loss_fn, optimizer):
    size = len(dataloader)
    # Get the batch from the dataset
    for batch, (X,y) in enumerate(dataloader):
        # Move data on GPU
        X_gpu = X.to(device)
        y_tensor = y.clone().detach()
        y_gpu = y_tensor.to(device)

        # Compute prediction and loss
        pred = model(X_gpu)
        loss = loss_fn(pred, y_gpu)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # Print loss during training
        if batch % 500 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f'loss: {loss} [{current:}/{size}]')

# Define the testing loop
def test_loop(dataloader, model, loss_fn):
    size = len(dataloader)
    num_batches = len(dataloader)
    test_loss, correct = 0,0

    # Disable weight update
    with torch.no_grad():
        for X,y in dataloader:
            pred = model(X)
            y_tensor = y.clone().detach()
            test_loss += loss_fn(pred, y_tensor).item()
            correct += (pred.argmax(1)).type(torch.float).sum().item()

            # Print the accuracy and the average loss
            test_loss = test_loss / num_batches
            correct = correct/size
            print(f'Accuracy: {correct * 100}, Average Loss: {test_loss}')



# Train the model
for t in range(epochs):
    print(f'Epoch: {t}')
    train_loop(train_dataloader, model, loss_fn, optimizer)
    print('Testing the model...')
    test_loop(test_dataloader, model, loss_fn)

# Save the model
torch.save(model.state_dict(), 'model.pth')
print('Saved PyTorch Model State to model.pth')

# Load the model
model.load_state_dict(torch.load('model.pth'))